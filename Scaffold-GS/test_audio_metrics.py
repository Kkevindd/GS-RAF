#!/usr/bin/env python3
"""
ç‹¬ç«‹æµ‹è¯•è„šæœ¬ï¼šæŒ‰ç…§NeRAF_evaluator.pyæ–¹æ³•è¯„ä¼°å®Œæ•´å£°å­¦æŒ‡æ ‡
ä½¿ç”¨NeRAFè¯„ä¼°å™¨è®¡ç®—T60ã€C50ã€EDTç­‰å£°å­¦æŒ‡æ ‡ï¼Œé‡‡ç”¨Griffin-Limé‡å»º
"""

import os
import sys
import json
import numpy as np
import torch
import torchaudio
from pathlib import Path
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
import logging
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import cv2

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from arguments import RAFDataParserConfig, RAFDataParser, RAFDataset
from nerfstudio.field_components.encodings import NeRFEncoding, SHEncoding
import torch.nn as nn
import torch.nn.functional as F
from scene import NeRAFAudioSoundField

import subprocess
cmd = 'nvidia-smi -q -d Memory |grep -A4 GPU|grep Used'
result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE).stdout.decode().split('\n')
os.environ['CUDA_VISIBLE_DEVICES']=str(np.argmin([int(x.split()[2]) for x in result[:-1]]))

os.system('echo $CUDA_VISIBLE_DEVICES')


# å¯¼å…¥å£°å­¦æŒ‡æ ‡è®¡ç®—å‡½æ•°
try:
    import pyroomacoustics
    from scipy.signal import hilbert
    PRA_AVAILABLE = True
except ImportError:
    PRA_AVAILABLE = False
    print("Warning: pyroomacoustics not available. Some acoustic metrics may not work.")

def setup_logger(log_path):
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_path)
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter("%(asctime)s - %(levelname)s: %(message)s")
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger


def measure_rt60_advance(signal, sr, decay_db=10, cutoff_freq=200):
    """é«˜çº§T60æµ‹é‡ï¼ˆæ¥è‡ªNeRAFï¼‰"""
    if not PRA_AVAILABLE:
        return -1
    
    signal = torch.from_numpy(signal)
    signal = torchaudio.functional.highpass_biquad(
        waveform=signal,
        sample_rate=sr,
        cutoff_freq=cutoff_freq
    )
    signal = signal.cpu().numpy()
    try:
        rt60 = pyroomacoustics.experimental.measure_rt60(signal, sr, decay_db=decay_db, plot=False)
        return rt60
    except:
        return -1

def compute_t60(true_in, gen_in, fs, advanced=True):
    """è®¡ç®—T60æŒ‡æ ‡"""
    ch = true_in.shape[0]
    gt = []
    pred = []
    for c in range(ch):
        try:
            if advanced: 
                true = measure_rt60_advance(true_in[c], sr=fs)
                gen = measure_rt60_advance(gen_in[c], sr=fs)
            else:
                if PRA_AVAILABLE:
                    true = pyroomacoustics.experimental.measure_rt60(true_in[c], fs=fs, decay_db=30)
                    gen = pyroomacoustics.experimental.measure_rt60(gen_in[c], fs=fs, decay_db=30)
                else:
                    true = -1
                    gen = -1
        except:
            true = -1
            gen = -1
        gt.append(true)
        pred.append(gen)
    return np.array(gt), np.array(pred)

def measure_clarity(signal, time=50, fs=44100):
    """æµ‹é‡C50æ¸…æ™°åº¦"""
    h2 = signal**2
    t = int((time/1000)*fs + 1) 
    return 10*np.log10(np.sum(h2[:t])/np.sum(h2[t:]))

def evaluate_clarity(pred_ir, gt_ir, fs):
    """è¯„ä¼°C50æ¸…æ™°åº¦"""
    ch = gt_ir.shape[0]
    gt = []
    pred = []
    for c in range(ch):
        pred_clarity = measure_clarity(pred_ir[c,...], fs=fs)
        gt_clarity = measure_clarity(gt_ir[c,...], fs=fs)
        gt.append(gt_clarity)
        pred.append(pred_clarity)
    return np.array(gt), np.array(pred)

def measure_edt(h, fs=44100, decay_db=10):
    """æµ‹é‡EDTæ—©æœŸè¡°å‡æ—¶é—´"""
    h = np.array(h)
    fs = float(fs)
    power = h ** 2
    energy = np.cumsum(power[::-1])[::-1]
    if np.all(energy == 0):
        return np.nan
    i_nz = np.max(np.where(energy > 0)[0])
    energy = energy[:i_nz]
    energy_db = 10 * np.log10(energy)
    energy_db -= energy_db[0]
    i_decay = np.min(np.where(- decay_db - energy_db > 0)[0])
    t_decay = i_decay / fs
    decay_time = t_decay
    est_edt = (60 / decay_db) * decay_time 
    return est_edt

def evaluate_edt(pred_ir, gt_ir, fs):
    """è¯„ä¼°EDTæ—©æœŸè¡°å‡æ—¶é—´"""
    ch = gt_ir.shape[0]
    gt = []
    pred = []
    for c in range(ch):
        pred_edt = measure_edt(pred_ir[c], fs=fs)
        gt_edt = measure_edt(gt_ir[c], fs=fs)
        gt.append(gt_edt)
        pred.append(pred_edt)
    return np.array(gt), np.array(pred)

def create_visualization_images(pred_stft, gt_stft, pred_waveform, gt_waveform, sample_idx, output_dir, logger):
    """åˆ›å»ºGTå’Œé¢„æµ‹ç»“æœçš„å¯è§†åŒ–å›¾åƒï¼ˆæŒ‰ç…§NeRAF_model.pyçš„æ–¹æ³•ï¼‰"""
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        vis_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(vis_dir, exist_ok=True)
        
        # 1. STFTé¢‘è°±å›¾å¯è§†åŒ–
        # è½¬æ¢ä¸ºnumpyæ ¼å¼
        pred_stft_np = pred_stft.cpu().numpy()  # [F, T]
        gt_stft_np = gt_stft.cpu().numpy()  # [F, T]
        
        # ç¡®ä¿STFTå¹…åº¦è°±ä¸ä¸ºè´Ÿï¼ˆè½¬æ¢ä¸ºå¹…åº¦è°±è¿›è¡Œå¯è§†åŒ–ï¼‰
        pred_mag_stft = np.maximum(np.exp(pred_stft_np) - 1e-3, 1e-6)  # è½¬æ¢ä¸ºå¹…åº¦è°±
        gt_mag_stft = np.maximum(np.exp(gt_stft_np) - 1e-3, 1e-6)  # è½¬æ¢ä¸ºå¹…åº¦è°±
        
        # è®¡ç®—å½’ä¸€åŒ–èŒƒå›´ï¼ˆæŒ‰ç…§NeRAFæ–¹æ³•ï¼‰
        min_val = min(pred_mag_stft.min(), gt_mag_stft.min())
        max_val = max(pred_mag_stft.max(), gt_mag_stft.max())
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        pred_norm = (pred_mag_stft - min_val) / (max_val - min_val)
        gt_norm = (gt_mag_stft - min_val) / (max_val - min_val)
        
        # åº”ç”¨viridisé¢œè‰²æ˜ å°„
        pred_colored = cm.viridis(pred_norm)[..., :3]  # [F, T, 3]
        gt_colored = cm.viridis(gt_norm)[..., :3]  # [F, T, 3]
        
        # è®¡ç®—å·®å¼‚å›¾ï¼ˆåŸºäºå¹…åº¦è°±ï¼‰
        diff_mag = pred_mag_stft - gt_mag_stft
        diff_norm = (diff_mag - min_val) / (max_val - min_val)
        diff_colored = cm.viridis(diff_norm)[..., :3]  # [F, T, 3]
        
        # åˆ›å»ºå¯¹æ¯”å›¾ï¼ˆé¢„æµ‹|GTï¼‰
        comparison_stft = np.concatenate([pred_colored, gt_colored], axis=1)  # [F, 2T, 3]
        
        # ä¿å­˜STFTå¯¹æ¯”å›¾
        stft_path = os.path.join(vis_dir, f"sample_{sample_idx:04d}_stft_comparison.png")
        plt.figure(figsize=(12, 8))
        plt.imshow(comparison_stft.transpose(1, 0, 2))  # è½¬ç½®ä»¥æ­£ç¡®æ˜¾ç¤º
        plt.title(f"Sample {sample_idx}: STFT Comparison (Predicted | Ground Truth)")
        plt.xlabel("Time")
        plt.ylabel("Frequency")
        plt.colorbar(label="Log Magnitude")
        plt.tight_layout()
        plt.savefig(stft_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜å·®å¼‚å›¾
        diff_path = os.path.join(vis_dir, f"sample_{sample_idx:04d}_stft_diff.png")
        plt.figure(figsize=(8, 6))
        plt.imshow(diff_colored.transpose(1, 0, 2))
        plt.title(f"Sample {sample_idx}: STFT Difference (Predicted - Ground Truth)")
        plt.xlabel("Time")
        plt.ylabel("Frequency")
        plt.colorbar(label="Log Magnitude Difference")
        plt.tight_layout()
        plt.savefig(diff_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # 2. æ³¢å½¢å¯è§†åŒ–
        # è½¬æ¢ä¸ºnumpyæ ¼å¼
        pred_wav_np = pred_waveform.cpu().numpy() if torch.is_tensor(pred_waveform) else pred_waveform
        gt_wav_np = gt_waveform.cpu().numpy() if torch.is_tensor(gt_waveform) else gt_waveform
        
        # åˆ›å»ºæ—¶é—´è½´
        fs = 48000  # å‡è®¾é‡‡æ ·ç‡
        time_axis = np.arange(len(pred_wav_np)) / fs
        
        # æ³¢å½¢å¯¹æ¯”å›¾
        plt.figure(figsize=(15, 10))
        
        # å­å›¾1ï¼šé¢„æµ‹æ³¢å½¢
        plt.subplot(3, 1, 1)
        plt.plot(time_axis, pred_wav_np, 'b-', linewidth=0.8)
        plt.title(f"Sample {sample_idx}: Predicted Waveform")
        plt.xlabel("Time (s)")
        plt.ylabel("Amplitude")
        plt.grid(True, alpha=0.3)
        
        # å­å›¾2ï¼šGTæ³¢å½¢
        plt.subplot(3, 1, 2)
        plt.plot(time_axis, gt_wav_np, 'r-', linewidth=0.8)
        plt.title(f"Sample {sample_idx}: Ground Truth Waveform")
        plt.xlabel("Time (s)")
        plt.ylabel("Amplitude")
        plt.grid(True, alpha=0.3)
        
        # å­å›¾3ï¼šå¯¹æ¯”å›¾
        plt.subplot(3, 1, 3)
        plt.plot(time_axis, pred_wav_np, 'b-', linewidth=0.8, label='Predicted', alpha=0.7)
        plt.plot(time_axis, gt_wav_np, 'r-', linewidth=0.8, label='Ground Truth', alpha=0.7)
        plt.title(f"Sample {sample_idx}: Waveform Comparison")
        plt.xlabel("Time (s)")
        plt.ylabel("Amplitude")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        waveform_path = os.path.join(vis_dir, f"sample_{sample_idx:04d}_waveform_comparison.png")
        plt.savefig(waveform_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # 3. é¢‘è°±åˆ†æå›¾
        plt.figure(figsize=(12, 8))
        
        # è®¡ç®—FFT
        pred_fft = np.fft.fft(pred_wav_np)
        gt_fft = np.fft.fft(gt_wav_np)
        freqs = np.fft.fftfreq(len(pred_wav_np), 1/fs)
        
        # åªå–æ­£é¢‘ç‡éƒ¨åˆ†
        pos_freqs = freqs[:len(freqs)//2]
        pred_mag = np.abs(pred_fft[:len(freqs)//2])
        gt_mag = np.abs(gt_fft[:len(freqs)//2])
        
        # ç¡®ä¿å¹…åº¦è°±ä¸ä¸ºè´Ÿï¼ˆè™½ç„¶FFTå¹…åº¦é€šå¸¸ä¸ä¸ºè´Ÿï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼‰
        pred_mag = np.maximum(pred_mag, 1e-6)
        gt_mag = np.maximum(gt_mag, 1e-6)
        
        mag_mse = np.mean((pred_mag - gt_mag) ** 2)
        print("mag_mse:",mag_mse)
        
        plt.subplot(2, 1, 1)
        plt.semilogy(pos_freqs, pred_mag, 'b-', linewidth=0.8, label='Predicted')
        plt.semilogy(pos_freqs, gt_mag, 'r-', linewidth=0.8, label='Ground Truth')
        plt.title(f"Sample {sample_idx}: Frequency Spectrum Comparison")
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Magnitude")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ç›¸ä½è°±
        pred_phase = np.angle(pred_fft[:len(freqs)//2])
        gt_phase = np.angle(gt_fft[:len(freqs)//2])
        
        phase_mse = np.mean((pred_phase - gt_phase) ** 2)
        print("phase_mse:",phase_mse)
        
        plt.subplot(2, 1, 2)
        plt.plot(pos_freqs, pred_phase, 'b-', linewidth=0.8, label='Predicted')
        plt.plot(pos_freqs, gt_phase, 'r-', linewidth=0.8, label='Ground Truth')
        plt.title(f"Sample {sample_idx}: Phase Spectrum Comparison")
        plt.xlabel("Frequency (Hz)")
        plt.ylabel("Phase (rad)")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        spectrum_path = os.path.join(vis_dir, f"sample_{sample_idx:04d}_spectrum_comparison.png")
        plt.savefig(spectrum_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Visualization images saved for sample {sample_idx}")
        return {
            'stft_comparison': stft_path,
            'stft_diff': diff_path,
            'waveform_comparison': waveform_path,
            'spectrum_comparison': spectrum_path
        }
        
    except Exception as e:
        logger.warning(f"Failed to create visualization for sample {sample_idx}: {e}")
        return None

def extract_local_features_from_grid(full_grid, speaker_pose, listener_pose, min_bound, max_bound, pooling_net=None, local_size=9, device='cuda'):
    """ä»å®Œæ•´ç½‘æ ¼ä¸­æå–å±€éƒ¨ç‰¹å¾
    
    å‚æ•°:
        full_grid: [35, 128, 128, 128] å®Œæ•´ç½‘æ ¼
        speaker_pose: [3] æ‰¬å£°å™¨ä½ç½®
        listener_pose: [3] å¬è€…ä½ç½®  
        min_bound: [3] ç½‘æ ¼æœ€å°è¾¹ç•Œ
        max_bound: [3] ç½‘æ ¼æœ€å¤§è¾¹ç•Œ
        pooling_net: æ± åŒ–ç½‘ç»œï¼ˆå¯é€‰ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨å¹³å‡æ± åŒ–ï¼‰
        local_size: int, å±€éƒ¨çª—å£å¤§å°ï¼ˆé»˜è®¤9ï¼‰
        device: è®¾å¤‡
    
    è¿”å›:
        [70] å±€éƒ¨ç‰¹å¾ï¼ˆæ‰¬å£°å™¨9Â³ + å¬è€…9Â³æ‹¼æ¥åæ± åŒ–ï¼‰
    """
    grid_size = 128
    half_size = local_size // 2
    
    # è®¡ç®—ä½“ç´ å¤§å°
    voxel_size = (max_bound - min_bound) / grid_size
    
    # å°†ä½ç½®è½¬æ¢ä¸ºç½‘æ ¼ç´¢å¼•
    speaker_idx = ((speaker_pose - min_bound) / voxel_size).long().clamp(0, grid_size - 1)
    listener_idx = ((listener_pose - min_bound) / voxel_size).long().clamp(0, grid_size - 1)
    
    # æå–æ‰¬å£°å™¨å‘¨å›´9Ã—9Ã—9
    speaker_min = (speaker_idx - half_size).clamp(0, grid_size - local_size)
    speaker_max = speaker_min + local_size
    speaker_local = full_grid[
        :,
        speaker_min[0]:speaker_max[0],
        speaker_min[1]:speaker_max[1],
        speaker_min[2]:speaker_max[2]
    ]  # [35, 9, 9, 9]
    
    # æå–å¬è€…å‘¨å›´9Ã—9Ã—9
    listener_min = (listener_idx - half_size).clamp(0, grid_size - local_size)
    listener_max = listener_min + local_size
    listener_local = full_grid[
        :,
        listener_min[0]:listener_max[0],
        listener_min[1]:listener_max[1],
        listener_min[2]:listener_max[2]
    ]  # [35, 9, 9, 9]
    
    # æ‹¼æ¥ï¼š[70, 9, 9, 9]
    combined = torch.cat([speaker_local, listener_local], dim=0)
    
    # æ± åŒ–
    if pooling_net is not None:
        # ä½¿ç”¨è®­ç»ƒçš„æ± åŒ–ç½‘ç»œ
        combined_batch = combined.unsqueeze(0)  # [1, 70, 9, 9, 9]
        pooled = pooling_net(combined_batch)  # [1, 70, 1, 1, 1]
        pooled = pooled.view(-1)  # [70]
    else:
        # ç®€å•å¹³å‡æ± åŒ–ï¼š[70, 9, 9, 9] -> [70]
        pooled = combined.mean(dim=[1, 2, 3])  # [70]
    
    return pooled

def load_audio_model(checkpoint_path, device, use_local_features=True):
    """åŠ è½½éŸ³é¢‘æ¨¡å‹
    
    å‚æ•°:
        checkpoint_path: éŸ³é¢‘æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡
        use_local_features: æ˜¯å¦ä½¿ç”¨å±€éƒ¨ç‰¹å¾æå–ï¼ˆä»å®Œæ•´ç½‘æ ¼ä¸­æå–ï¼‰
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # é‡å»ºç¼–ç å™¨
    time_enc = NeRFEncoding(in_dim=1, num_frequencies=10, min_freq_exp=0.0, max_freq_exp=8.0, include_input=True)
    pos_enc = NeRFEncoding(in_dim=3, num_frequencies=10, min_freq_exp=0.0, max_freq_exp=8.0, include_input=True)
    rot_enc = SHEncoding(levels=4, implementation="tcnn")
    
    time_dim = time_enc.get_out_dim()
    pos_dim = pos_enc.get_out_dim()
    rot_dim = rot_enc.get_out_dim()
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨å±€éƒ¨ç‰¹å¾ç¡®å®šgrid_feat_dim
    pooling_net = None
    if use_local_features:
        grid_feat_dim = 70  # å±€éƒ¨ç‰¹å¾ç»´åº¦
        print(f"âœ“ Using LOCAL features (70-dim) extracted from full grid")
        if checkpoint.get("local_pooling_net") is not None:
                # éœ€è¦å¯¼å…¥LocalPoolingNetç±»
                try:
                    from scene.gaussian_model import LocalPoolingNet
                    pooling_net = LocalPoolingNet().to(device)
                    pooling_net.load_state_dict(checkpoint["local_pooling_net"])
                    pooling_net.eval()
                    print(f"âœ“ Loaded local_pooling_net from checkpoint")
                except Exception as e:
                    print(f"âš  Failed to load LocalPoolingNet class: {e}")
                    print(f"âš  Will use average pooling instead")
                    pooling_net = None
        else:
            print(f"âš  local_pooling_net not found in checkpoint, will use average pooling")
    else:
        # ä½¿ç”¨å®Œæ•´ç½‘æ ¼ç‰¹å¾
        full_grid = checkpoint["grid_feature"]
        if isinstance(full_grid, np.ndarray):
            full_grid_shape = full_grid.shape
        else:
            full_grid_shape = tuple(full_grid.size()) if hasattr(full_grid, 'size') else 'unknown'
        
        # å¦‚æœæ˜¯å®Œæ•´ç½‘æ ¼[35,128,128,128]ï¼Œflattenå®ƒ
        if full_grid_shape == (35, 128, 128, 128):
            grid_feat_dim = 35 * 128 * 128 * 128
            print(f"âœ“ Using full grid (will be flattened from [35,128,128,128] to {grid_feat_dim}-dim)")
        else:
            grid_feat_dim = full_grid.size if isinstance(full_grid, np.ndarray) else len(full_grid)
            print(f"âœ“ Using GLOBAL features from checkpoint ({grid_feat_dim}-dim)")
    
    # é‡å»ºéŸ³é¢‘ç½‘ç»œ
    audio_W = 512
    audio_F = 513
    in_size = grid_feat_dim + time_dim + 2 * pos_dim + rot_dim
    
    print(f"Audio network input size: {in_size} (grid_feat: {grid_feat_dim}, time: {time_dim}, pos: {pos_dim}Ã—2, rot: {rot_dim})")
    
    audio_field = NeRAFAudioSoundField(in_size=in_size, W=audio_W, sound_rez=1, N_frequencies=audio_F)
    
    # åŠ è½½æƒé‡
    if checkpoint.get("time_enc") is not None:
        time_enc.load_state_dict(checkpoint["time_enc"])
    if checkpoint.get("pos_enc") is not None:
        pos_enc.load_state_dict(checkpoint["pos_enc"])
    if checkpoint.get("rot_enc") is not None:
        rot_enc.load_state_dict(checkpoint["rot_enc"])
    
    audio_field.load_state_dict(checkpoint["audio_field"])
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    time_enc = time_enc.to(device)
    pos_enc = pos_enc.to(device)
    rot_enc = rot_enc.to(device)
    audio_field = audio_field.to(device)
    audio_field.eval()
    
    return time_enc, pos_enc, rot_enc, audio_field, checkpoint, pooling_net

def evaluate_audio_field(model_path, raf_data_root, checkpoint_name=None, max_samples=None, logger=None, save_visualizations=True, max_vis_samples=10, use_local_features=True):
    """æŒ‰ç…§NeRAF_evaluator.pyæ–¹æ³•è¯„ä¼°å®Œæ•´å£°å­¦æŒ‡æ ‡
    
    å‚æ•°:
        use_local_features: æ˜¯å¦ä½¿ç”¨å±€éƒ¨ç‰¹å¾æå–ï¼ˆé»˜è®¤Trueï¼‰
    """
    
    if logger is None:
        logger = setup_logger(os.path.join(model_path, "audio_metrics_test.log"))
    
    logger.info("=" * 80)
    logger.info("AUDIO FIELD EVALUATION (NeRAF Method)")
    logger.info("=" * 80)
    logger.info(f"Model path: {model_path}")
    logger.info(f"RAF data: {raf_data_root}")
    logger.info(f"Use local features: {use_local_features}")
    
    # 1. æŸ¥æ‰¾éŸ³é¢‘æ£€æŸ¥ç‚¹
    audio_ckpt_dir = os.path.join(model_path, "audio_ckpts")
    if not os.path.exists(audio_ckpt_dir):
        logger.error(f"Audio checkpoint directory not found: {audio_ckpt_dir}")
        return
    
    audio_ckpt_files = [f for f in os.listdir(audio_ckpt_dir) if f.startswith('audio_') and f.endswith('.pth')]
    if not audio_ckpt_files:
        logger.error(f"No audio checkpoint files found in {audio_ckpt_dir}")
        return
    
    # é€‰æ‹©æ£€æŸ¥ç‚¹
    if checkpoint_name:
        ckpt_path = os.path.join(audio_ckpt_dir, checkpoint_name)
        if not os.path.exists(ckpt_path):
            logger.error(f"Specified checkpoint not found: {ckpt_path}")
            return
    else:
        # é€‰æ‹©æœ€æ–°çš„æ£€æŸ¥ç‚¹
        audio_ckpt_files.sort()
        latest_ckpt = audio_ckpt_files[-1]
        ckpt_path = os.path.join(audio_ckpt_dir, latest_ckpt)
    
    logger.info(f"Using checkpoint: {os.path.basename(ckpt_path)}")
    
    # 2. åŠ è½½æ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Using device: {device}")
    
    try:
        time_enc, pos_enc, rot_enc, audio_field, checkpoint, pooling_net = load_audio_model(
            ckpt_path, device, use_local_features=use_local_features
        )
        logger.info("Model loaded successfully")
        if pooling_net is not None:
            logger.info("âœ“ Using trained local_pooling_net for feature extraction")
        elif use_local_features:
            logger.info("âš  Using simple average pooling (local_pooling_net not found)")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        return
    
    # 3. è·å–å‚æ•°
    raf_fs = checkpoint.get("raf_fs", 48000)
    raf_max_len_s = checkpoint.get("raf_max_len_s", 0.32)
    hop_len = checkpoint.get("hop_len", 256)
    max_len_frames = checkpoint.get("max_len_frames", 60)
    
    logger.info(f"RAF parameters: fs={raf_fs}, max_len_s={raf_max_len_s}, hop_len={hop_len}")
    
    # 4. è®¾ç½®STFTå‚æ•°ï¼ˆæŒ‰ç…§NeRAFæ–¹æ³•ï¼‰
    if raf_fs == 48000:
        n_fft = 1024
        win_length = 512
        hop_length = 256
    elif raf_fs == 16000:
        n_fft = 512
        win_length = 256
        hop_length = 128
    else:
        logger.error(f"Unsupported sample rate: {raf_fs}")
        return
    
    # 5. åˆ›å»ºNeRAFè¯„ä¼°å™¨
    try:
        # å¯¼å…¥NeRAFè¯„ä¼°å™¨
        sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'NeRAF'))
        from NeRAF.NeRAF_evaluator import RAFEvaluator
        from NeRAF.NeRAF_evaluator import STFTLoss
        
        evaluator = RAFEvaluator(fs=raf_fs)
        logger.info("NeRAF evaluator initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize NeRAF evaluator: {e}")
        return
    
    # 6. åŠ è½½è¯„ä¼°æ•°æ® - ä½¿ç”¨å®Œæ•´éŸ³é¢‘è¯„ä¼°
    try:
        dp_cfg = RAFDataParserConfig(data=Path(raf_data_root))
        dp = RAFDataParser(dp_cfg)
        dpo_test = dp.get_dataparser_outputs(split="test")
        
        # åˆ›å»ºå®Œæ•´éŸ³é¢‘è¯„ä¼°æ•°æ®é›†
        raf_eval_ds = RAFDataset(
            dataparser_outputs=dpo_test,
            mode='eval_image',  # å®Œæ•´éŸ³é¢‘è¯„ä¼°ï¼ŒåŒ…å«æ³¢å½¢æ•°æ®
            max_len=max_len_frames,
            max_len_time=raf_max_len_s,
            wav_path=os.path.join(raf_data_root, 'data'),
            fs=raf_fs, 
            hop_len=hop_len,
        )
        
        logger.info(f"Loaded evaluation dataset: {len(raf_eval_ds)} samples")
        
    except Exception as e:
        logger.error(f"Failed to load evaluation dataset: {e}")
        return
    
    
    # 7. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if max_samples is None:
        # è¯„ä¼°æ‰€æœ‰æ ·æœ¬
        eval_samples = len(raf_eval_ds)
        eval_loader = DataLoader(raf_eval_ds, batch_size=1, shuffle=False, num_workers=0)
        logger.info(f"Evaluating ALL {eval_samples} samples...")
    else:
        # è¯„ä¼°æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        eval_samples = min(max_samples, len(raf_eval_ds))
        eval_subset = torch.utils.data.Subset(raf_eval_ds, list(range(eval_samples)))
        eval_loader = DataLoader(eval_subset, batch_size=1, shuffle=False, num_workers=0)
        logger.info(f"Evaluating {eval_samples} samples (out of {len(raf_eval_ds)} total)...")
    
    # 8. æ‰§è¡Œè¯„ä¼° - æŒ‰ç…§NeRAFæ–¹æ³•è®¡ç®—å®Œæ•´å£°å­¦æŒ‡æ ‡
    all_metrics = []
    
    for batch_idx, batch in enumerate(tqdm(eval_loader, desc="Evaluating audio metrics")):
        try:
            with torch.no_grad():
                # è·å–çœŸå®æ•°æ®
                gt_stft = batch['data'].to(device).float()  # [1, F, T]
                gt_waveform = batch['waveform'].to(device).float()  # [1, T]
                
                # é‡å»ºå®Œæ•´STFT
                # max_len = gt_stft.shape[-1]
                # time_query = torch.arange(0, max_len, 1, device=device).unsqueeze(-1).float() / float(max_len - 1.0)
                
                max_len_frames = checkpoint.get("max_len_frames", 60)  # ä»checkpointè·å–
                time_query = torch.arange(0, max_len_frames, 1, device=device).unsqueeze(-1).float() / float(max_len_frames - 1.0)
                
                t_feat = time_enc(time_query)
                
                # ä½¿ç”¨ä¿å­˜çš„è¾¹ç•Œä¿¡æ¯
                min_bound = torch.tensor(checkpoint["aabb_min"]).to(device)
                max_bound = torch.tensor(checkpoint["aabb_max"]).to(device)
                extent = (max_bound - min_bound).clamp_min(1e-6)
                
                mic_pose = batch['mic_pose'].to(device).float()
                src_pose = batch['source_pose'].to(device).float()
                mic01 = ((mic_pose - min_bound) / extent).clamp(0.0, 1.0)
                src01 = ((src_pose - min_bound) / extent).clamp(0.0, 1.0)
                mic_feat = pos_enc(mic01)
                src_feat = pos_enc(src01)
                
                rot = batch['rot'].to(device).float()
                rot_feat = rot_enc(rot)
                
                # æå–ç½‘æ ¼ç‰¹å¾ï¼šå±€éƒ¨ç‰¹å¾æˆ–å…¨å±€ç‰¹å¾
                if use_local_features:
                    # ä»å®Œæ•´ç½‘æ ¼ä¸­æå–å±€éƒ¨ç‰¹å¾
                    # åŠ è½½å®Œæ•´ç½‘æ ¼ [35, 128, 128, 128]
                    full_grid = torch.tensor(checkpoint["grid_feature"]).to(device)
                    # æ³¨æ„ï¼šcheckpointä¿å­˜çš„å¯èƒ½æ˜¯flattenedçš„ï¼Œéœ€è¦reshape
                    if full_grid.dim() == 1:
                        # å¦‚æœæ˜¯flattençš„ï¼Œreshapeå›[35, 128, 128, 128]
                        full_grid = full_grid.view(35, 128, 128, 128)
                    
                    min_bound = torch.tensor(checkpoint["aabb_min"]).to(device)
                    max_bound = torch.tensor(checkpoint["aabb_max"]).to(device)
                    
                    # æå–å±€éƒ¨ç‰¹å¾ [70]ï¼ˆä½¿ç”¨è®­ç»ƒçš„pooling_netæˆ–å¹³å‡æ± åŒ–ï¼‰
                    local_feat = extract_local_features_from_grid(
                        full_grid,
                        src_pose.squeeze(0),  # [1, 3] -> [3]
                        mic_pose.squeeze(0),  # [1, 3] -> [3]
                        min_bound,
                        max_bound,
                        pooling_net=pooling_net,  # ä¼ é€’poolingç½‘ç»œ
                        local_size=9,
                        device=device
                    )  # [70]
                    
                    # æ‰©å±•åˆ°æ‰€æœ‰æ—¶é—´å¸§
                    B = t_feat.shape[0]
                    grid_feat = local_feat.unsqueeze(0).expand(B, -1)  # [1, 70] -> [B, 70]
                else:
                    # ä½¿ç”¨ä¿å­˜çš„å…¨å±€ç‰¹å¾
                    grid_feat = torch.tensor(checkpoint["grid_feature"]).to(device)
                    # å¦‚æœæ˜¯å¤šç»´çš„ï¼Œflatten
                    if grid_feat.dim() > 1:
                        grid_feat = grid_feat.flatten()
                    B = t_feat.shape[0]
                    grid_feat = grid_feat.unsqueeze(0).expand(B, -1)  # [grid_dim] -> [B, grid_dim]
                
                # æ‰©å±•å…¶ä»–ç‰¹å¾åˆ°æ‰€æœ‰æ—¶é—´å¸§
                mic_feat = mic_feat.expand(B, -1)
                src_feat = src_feat.expand(B, -1)
                rot_feat = rot_feat.expand(B, -1)
                
                h = torch.cat([grid_feat, t_feat, mic_feat, src_feat, rot_feat], dim=-1)
                field_outputs = audio_field(h)  # [B, C, F] where B=time_frames, C=channels, F=frequencies
                
                # è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼ [C, F, T]ï¼ˆæŒ‰ç…§NeRAFçš„å®ç°æ–¹å¼ï¼‰
                # field_outputs: [T, C, F] -> permute to [C, F, T]
                pred_log_stft = field_outputs.permute(1, 2, 0)  # [T, C, F] -> [C, F, T]
                
                # GT STFT: [1, 1, 513, 60] -> squeeze to [513, 60] -> unsqueeze to [1, 513, 60]
                gt_stft_squeezed = gt_stft.squeeze(0).squeeze(0)  # [513, 60]
                gt_stft_tensor = gt_stft_squeezed.unsqueeze(0)  # [1, 513, 60] -> [C, F, T]
                
                # è®¡ç®—STFTè¯¯å·®ï¼ˆåœ¨[C, F, T]æ ¼å¼ä¸‹ï¼‰
                stft_mse = torch.mean((pred_log_stft - gt_stft_tensor) ** 2).item()
                stft_l1 = torch.mean(torch.abs(pred_log_stft - gt_stft_tensor)).item()
                
                
                
                # é‡å»ºæ³¢å½¢ç”¨äºå£°å­¦æŒ‡æ ‡è®¡ç®—ï¼ˆæŒ‰ç…§NeRAFæ–¹æ³•ï¼‰
                # ç¡®ä¿å¹…åº¦è°±ä¸ä¸ºè´Ÿ
                pred_mag_stft = torch.clamp(torch.exp(pred_log_stft) - 1e-3, min=1e-6, max=10000.0)  # [C, F, T]
                
                # ä½¿ç”¨Griffin-Limé‡å»ºæ³¢å½¢ï¼ˆå®Œå…¨æŒ‰ç…§NeRAFçš„å‚æ•°è®¾ç½®ï¼‰
                # å…³é”®ä¿®å¤ï¼šNeRAFä½¿ç”¨ n_fft = (N_freq_stft-1)*2 = (513-1)*2 = 1024
                griffin_lim = torchaudio.transforms.GriffinLim(
                    n_fft=(513-1)*2,  # ä¿®å¤ï¼šä½¿ç”¨ (513-1)*2 = 1024ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨n_fft
                    win_length=win_length,
                    hop_length=hop_length,
                    power=1  # å…³é”®ï¼šNeRAFä½¿ç”¨power=1ï¼Œä¸è®¾ç½®å…¶ä»–å‚æ•°
                ).to(device)
                
                pred_waveform = griffin_lim(pred_mag_stft)
                pred_waveform = pred_waveform.squeeze(0).cpu().numpy()  # [T]
                
                # GT waveform: [1, 1, T] -> squeeze to [T]
                gt_waveform_squeezed = gt_waveform.squeeze(0).squeeze(0).cpu().numpy()  # [T]
                
                # ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼ŒGTæ³¢å½¢ä¹Ÿåº”è¯¥é€šè¿‡ç›¸åŒçš„Griffin-Limé‡å»ºè¿‡ç¨‹
                # è¿™æ ·ç¡®ä¿é¢„æµ‹å’ŒGTéƒ½ç»è¿‡ç›¸åŒçš„é‡å»ºè¿‡ç¨‹
                # ç¡®ä¿GTå¹…åº¦è°±ä¹Ÿä¸ä¸ºè´Ÿ
                gt_mag_stft = torch.clamp(torch.exp(gt_stft_tensor) - 1e-3, min=1e-6, max=10000.0)  # [C, F, T]
                
                gt_waveform_reconstructed = griffin_lim(gt_mag_stft)
                gt_waveform_reconstructed = gt_waveform_reconstructed.squeeze(0).cpu().numpy()  # [T]
                
                # ç¡®ä¿æ³¢å½¢é•¿åº¦ä¸€è‡´
                min_len = min(len(pred_waveform), len(gt_waveform_reconstructed))
                pred_waveform = pred_waveform[:min_len]
                gt_waveform_squeezed = gt_waveform_reconstructed[:min_len]  # ä½¿ç”¨é‡å»ºåçš„GTæ³¢å½¢
                
                # è°ƒè¯•ä¿¡æ¯
                if batch_idx < 30:  # åªæ‰“å°å‰5ä¸ªæ ·æœ¬çš„è°ƒè¯•ä¿¡æ¯
                    logger.info(f"[Sample {batch_idx}] Pred waveform range: [{pred_waveform.min():.6f}, {pred_waveform.max():.6f}]")
                    logger.info(f"[Sample {batch_idx}] GT waveform (reconstructed) range: [{gt_waveform_squeezed.min():.6f}, {gt_waveform_squeezed.max():.6f}]")
                    # å¯¹æ¯”åŸå§‹GTæ³¢å½¢å’Œé‡å»ºGTæ³¢å½¢
                    gt_waveform_original = gt_waveform.squeeze(0).squeeze(0).cpu().numpy()
                    logger.info(f"[Sample {batch_idx}] GT waveform (original) range: [{gt_waveform_original.min():.6f}, {gt_waveform_original.max():.6f}]")
                    logger.info(f"[Sample {batch_idx}] GT reconstruction difference: {np.mean(np.abs(gt_waveform_original[:min_len] - gt_waveform_squeezed)):.6f}")
                
                # åˆ›å»ºå¯è§†åŒ–å›¾åƒï¼ˆä»…å¯¹å‰å‡ ä¸ªæ ·æœ¬ï¼‰
                vis_paths = None
                if save_visualizations and batch_idx < max_vis_samples:
                    vis_paths = create_visualization_images(
                        pred_log_stft.squeeze(0), gt_stft_squeezed,  # è½¬æ¢ä¸º[F, T]æ ¼å¼ç”¨äºå¯è§†åŒ–
                        pred_waveform, gt_waveform_squeezed,  # ä½¿ç”¨é‡å»ºåçš„GTæ³¢å½¢
                        batch_idx, model_path, logger
                    )
                
                # ä½¿ç”¨NeRAFè¯„ä¼°å™¨è®¡ç®—å®Œæ•´å£°å­¦æŒ‡æ ‡
                try:
                    # å‡†å¤‡æ•°æ®æ ¼å¼ï¼ˆæŒ‰ç…§NeRAF_model.pyçš„è¦æ±‚ï¼‰
                    # STFTæ•°æ®å·²ç»æ˜¯ [C, F, T] æ ¼å¼
                    pred_log_stft_tensor = pred_log_stft  # [C, F, T]
                    gt_stft_tensor = gt_stft_tensor  # [C, F, T]
                    
                    # è®¡ç®—å¹…åº¦è°± - ç¡®ä¿ä¸ä¸ºè´Ÿ
                    mag_prd = torch.clamp(torch.exp(pred_log_stft_tensor) - 1e-3, min=1e-6, max=10000.0)  # [C, F, T]
                    mag_gt = torch.clamp(torch.exp(gt_stft_tensor) - 1e-3, min=1e-6, max=10000.0)  # [C, F, T]
                    
                    # è½¬æ¢ä¸ºnumpyæ ¼å¼
                    mag_prd_np = mag_prd.cpu().numpy()
                    mag_gt_np = mag_gt.cpu().numpy()
                    
                    # æ³¢å½¢æ•°æ®æ ¼å¼ï¼š[channels, time]
                    wav_pred_istft = pred_waveform.reshape(1, -1)  # [1, T]
                    wav_gt_istft = gt_waveform_squeezed.reshape(1, -1)  # [1, T] - é‡å»ºåçš„GTæ³¢å½¢
                    # å¯¹äºå£°å­¦æŒ‡æ ‡è®¡ç®—ï¼Œä½¿ç”¨åŸå§‹GTæ³¢å½¢ï¼ˆæŒ‰ç…§NeRAFçš„åšæ³•ï¼‰
                    gt_waveform_original = gt_waveform.squeeze(0).squeeze(0).cpu().numpy()
                    wav_gt_ff = gt_waveform_original.reshape(1, -1)  # [1, T] - åŸå§‹GTæ³¢å½¢ç”¨äºå£°å­¦æŒ‡æ ‡
                    
                    # log STFTæ ¼å¼ [C, F, T]
                    log_prd = pred_log_stft_tensor.cpu().numpy()  # [C, F, T]
                    log_gt = gt_stft_tensor.cpu().numpy()  # [C, F, T]
                    
                    # ä½¿ç”¨NeRAFè¯„ä¼°å™¨
                    metrics = evaluator.get_full_metrics(
                        mag_prd_np, mag_gt_np, wav_gt_ff, wav_pred_istft, wav_gt_istft, log_prd, log_gt
                    )
                    
                    # æ·»åŠ STFTè¯¯å·®
                    metrics['stft_mse'] = stft_mse
                    metrics['stft_l1'] = stft_l1
                    
                except Exception as e:
                    logger.warning(f"Error computing NeRAF metrics for sample {batch_idx}: {e}")
                    # ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                    metrics = {
                        'stft_mse': stft_mse,
                        'stft_l1': stft_l1,
                        'audio_T60': 100.0,
                        'audio_total_invalids_T60': 1,
                        'audio_stft_error': 1.0,
                        'audio_EDT': 1.0,
                        'audio_C50': 1.0,
                    }
                
                all_metrics.append(metrics)
                
                if batch_idx % 10 == 0:
                    logger.info(f"Processed {batch_idx + 1}/{len(eval_loader)} samples")
                    
        except Exception as e:
            logger.error(f"Error evaluating sample {batch_idx}: {e}")
            continue
    
    # 9. è®¡ç®—å¹³å‡æŒ‡æ ‡
    if all_metrics:
        avg_metrics = {}
        for key in all_metrics[0].keys():
            values = [m[key] for m in all_metrics]
            avg_metrics[key] = np.mean(values)
            avg_metrics[f"{key}_std"] = np.std(values)
        
        # 10. ä¿å­˜ç»“æœ
        results_dir = os.path.join(model_path, "audio_neraf_results")
        os.makedirs(results_dir, exist_ok=True)
        
        checkpoint_basename = os.path.basename(ckpt_path).replace('.pth', '')
        results_file = os.path.join(results_dir, f"neraf_metrics_{checkpoint_basename}.json")
        
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        # è½¬æ¢æ‰€æœ‰æ•°æ®
        converted_avg_metrics = convert_numpy_types(avg_metrics)
        converted_all_samples = convert_numpy_types(all_metrics)
        
        with open(results_file, 'w') as f:
            json.dump({
                'checkpoint': checkpoint_basename,
                'num_samples': len(all_metrics),
                'avg_metrics': converted_avg_metrics,
                'all_samples': converted_all_samples
            }, f, indent=2)
        
        # 11. æ‰“å°ç»“æœ
        logger.info("=" * 80)
        logger.info("AUDIO FIELD EVALUATION RESULTS (NeRAF Method)")
        logger.info("=" * 80)
        logger.info(f"Checkpoint: {checkpoint_basename}")
        logger.info(f"Number of samples: {len(all_metrics)}")
        logger.info(f"Evaluation method: NeRAF evaluator with Griffin-Lim reconstruction")
        if save_visualizations:
            logger.info(f"Visualizations saved for first {min(max_vis_samples, len(all_metrics))} samples")
        logger.info("-" * 80)
        
        for key, value in avg_metrics.items():
            if not key.endswith('_std'):
                std_key = f"{key}_std"
                std_value = avg_metrics.get(std_key, 0)
                logger.info(f"{key:30s}: {value:.6f} Â± {std_value:.6f}")
        
        logger.info("=" * 80)
        logger.info(f"Results saved to: {results_file}")
        
        return avg_metrics
    else:
        logger.error("No valid samples evaluated")
        return None

def main():
    parser = argparse.ArgumentParser(description="Test audio metrics for trained models")
    parser.add_argument("--model_path", type=str, required=True, help="Path to the trained model directory or checkpoint file")
    parser.add_argument("--raf_data", type=str, required=True, help="Path to RAF dataset")
    parser.add_argument("--checkpoint", type=str, default=None, help="Specific checkpoint name (e.g., 'audio_iter_30000.pth')")
    parser.add_argument("--max_samples", type=int, default=None, help="Maximum number of samples to evaluate (default: all samples)")
    parser.add_argument("--output_dir", type=str, default=None, help="Output directory for results")
    parser.add_argument("--save_visualizations", action="store_true", default=True, help="Save visualization images")
    parser.add_argument("--max_vis_samples", type=int, default=10, help="Maximum number of samples to visualize")
    parser.add_argument("--use_local_features", action="store_true", default=True, help="Use local features (70-dim) instead of global (default: True)")
    parser.add_argument("--use_global_features", action="store_true", default=False, help="Use global features from checkpoint (overrides --use_local_features)")
    
    args = parser.parse_args()
    
    # æ™ºèƒ½å¤„ç†model_pathï¼šå¦‚æœä¼ å…¥çš„æ˜¯checkpointæ–‡ä»¶ï¼Œè‡ªåŠ¨æå–ç›®å½•
    original_model_path = args.model_path
    if os.path.isfile(args.model_path) and args.model_path.endswith('.pth'):
        # ç”¨æˆ·ä¼ å…¥äº†checkpointæ–‡ä»¶è·¯å¾„
        checkpoint_file = os.path.basename(args.model_path)
        # è·å–æ¨¡å‹ç›®å½•ï¼ˆå‘ä¸Šä¸¤çº§ï¼šaudio_ckpts/ -> model_dir/ï¼‰
        audio_ckpts_dir = os.path.dirname(args.model_path)
        args.model_path = os.path.dirname(audio_ckpts_dir)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šcheckpointï¼Œä½¿ç”¨ä¼ å…¥çš„æ–‡ä»¶
        if args.checkpoint is None:
            args.checkpoint = checkpoint_file
        
        print(f"ğŸ“Œ æ£€æµ‹åˆ°checkpointæ–‡ä»¶è·¯å¾„ï¼Œè‡ªåŠ¨è½¬æ¢ï¼š")
        print(f"   åŸå§‹è·¯å¾„: {original_model_path}")
        print(f"   æ¨¡å‹ç›®å½•: {args.model_path}")
        print(f"   Checkpoint: {args.checkpoint}")
        print()
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨å±€éƒ¨ç‰¹å¾
    use_local_features = args.use_local_features and not args.use_global_features
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        os.makedirs(args.output_dir, exist_ok=True)
        log_path = os.path.join(args.output_dir, "audio_metrics_test.log")
    else:
        log_path = os.path.join(args.model_path, "audio_metrics_test.log")
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(log_path)
    
    # æ‰§è¡Œè¯„ä¼°
    try:
        results = evaluate_audio_field(
            model_path=args.model_path,
            raf_data_root=args.raf_data,
            checkpoint_name=args.checkpoint,
            max_samples=args.max_samples,
            logger=logger,
            save_visualizations=args.save_visualizations,
            max_vis_samples=args.max_vis_samples,
            use_local_features=use_local_features
        )
        
        if results:
            logger.info("Evaluation completed successfully!")
        else:
            logger.error("Evaluation failed!")
            
    except Exception as e:
        logger.error(f"Evaluation failed with error: {e}")
        raise

if __name__ == "__main__":
    main()
